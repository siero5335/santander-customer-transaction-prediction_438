{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Denoise_autoencode_seed71.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "b33tg60If59L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Base: https://www.kaggle.com/rspadim/simple-denoise-autoencoder-with-keras"
      ]
    },
    {
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "TphHCz-RQGP-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8ca34351-7fff-49c7-d51b-d729485179dd"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import keras \n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import Sequence\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Input, Concatenate, Dropout\n",
        "from pathlib import Path"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "MOJI8tdGQmf8",
        "colab_type": "code",
        "outputId": "d67d76b9-e2cb-45ff-b90d-f35cc89785e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['.config', 'train.csv', 'test.csv', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MQGgcMI4Zepy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "INT8_MIN    = np.iinfo(np.int8).min\n",
        "INT8_MAX    = np.iinfo(np.int8).max\n",
        "INT16_MIN   = np.iinfo(np.int16).min\n",
        "INT16_MAX   = np.iinfo(np.int16).max\n",
        "INT32_MIN   = np.iinfo(np.int32).min\n",
        "INT32_MAX   = np.iinfo(np.int32).max\n",
        "\n",
        "FLOAT16_MIN = np.finfo(np.float16).min\n",
        "FLOAT16_MAX = np.finfo(np.float16).max\n",
        "FLOAT32_MIN = np.finfo(np.float32).min\n",
        "FLOAT32_MAX = np.finfo(np.float32).max\n",
        "\n",
        "def memory_usage(data, detail=1):\n",
        "    if detail:\n",
        "        display(data.memory_usage())\n",
        "    memory = data.memory_usage().sum() / (1024*1024)\n",
        "    print(\"Memory usage : {0:.2f}MB\".format(memory))\n",
        "    return memory\n",
        "\n",
        "def compress_dataset(data):\n",
        "    \"\"\"\n",
        "        Compress datatype as small as it can\n",
        "        Parameters\n",
        "        ----------\n",
        "        path: pandas Dataframe\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            None\n",
        "    \"\"\"\n",
        "    memory_before_compress = memory_usage(data, 0)\n",
        "    print()\n",
        "    length_interval      = 50\n",
        "    length_float_decimal = 4\n",
        "\n",
        "    print('='*length_interval)\n",
        "    for col in data.columns:\n",
        "        col_dtype = data[col][:100].dtype\n",
        "\n",
        "        if col_dtype != 'object':\n",
        "            print(\"Name: {0:24s} Type: {1}\".format(col, col_dtype))\n",
        "            col_series = data[col]\n",
        "            col_min = col_series.min()\n",
        "            col_max = col_series.max()\n",
        "\n",
        "            if col_dtype == 'float64':\n",
        "                print(\" variable min: {0:15s} max: {1:15s}\".format(str(np.round(col_min, length_float_decimal)), str(np.round(col_max, length_float_decimal))))\n",
        "                if (col_min > FLOAT16_MIN) and (col_max < FLOAT16_MAX):\n",
        "                    data[col] = data[col].astype(np.float16)\n",
        "                    print(\"  float16 min: {0:15s} max: {1:15s}\".format(str(FLOAT16_MIN), str(FLOAT16_MAX)))\n",
        "                    print(\"compress float64 --> float16\")\n",
        "                elif (col_min > FLOAT32_MIN) and (col_max < FLOAT32_MAX):\n",
        "                    data[col] = data[col].astype(np.float32)\n",
        "                    print(\"  float32 min: {0:15s} max: {1:15s}\".format(str(FLOAT32_MIN), str(FLOAT32_MAX)))\n",
        "                    print(\"compress float64 --> float32\")\n",
        "                else:\n",
        "                    pass\n",
        "                memory_after_compress = memory_usage(data, 0)\n",
        "                print(\"Compress Rate: [{0:.2%}]\".format((memory_before_compress-memory_after_compress) / memory_before_compress))\n",
        "                print('='*length_interval)\n",
        "\n",
        "            if col_dtype == 'int64':\n",
        "                print(\" variable min: {0:15s} max: {1:15s}\".format(str(col_min), str(col_max)))\n",
        "                type_flag = 64\n",
        "                if (col_min > INT8_MIN/2) and (col_max < INT8_MAX/2):\n",
        "                    type_flag = 8\n",
        "                    data[col] = data[col].astype(np.int8)\n",
        "                    print(\"     int8 min: {0:15s} max: {1:15s}\".format(str(INT8_MIN), str(INT8_MAX)))\n",
        "                elif (col_min > INT16_MIN) and (col_max < INT16_MAX):\n",
        "                    type_flag = 16\n",
        "                    data[col] = data[col].astype(np.int16)\n",
        "                    print(\"    int16 min: {0:15s} max: {1:15s}\".format(str(INT16_MIN), str(INT16_MAX)))\n",
        "                elif (col_min > INT32_MIN) and (col_max < INT32_MAX):\n",
        "                    type_flag = 32\n",
        "                    data[col] = data[col].astype(np.int32)\n",
        "                    print(\"    int32 min: {0:15s} max: {1:15s}\".format(str(INT32_MIN), str(INT32_MAX)))\n",
        "                    type_flag = 1\n",
        "                else:\n",
        "                    pass\n",
        "                memory_after_compress = memory_usage(data, 0)\n",
        "                print(\"Compress Rate: [{0:.2%}]\".format((memory_before_compress-memory_after_compress) / memory_before_compress))\n",
        "                if type_flag == 32:\n",
        "                    print(\"compress (int64) ==> (int32)\")\n",
        "                elif type_flag == 16:\n",
        "                    print(\"compress (int64) ==> (int16)\")\n",
        "                else:\n",
        "                    print(\"compress (int64) ==> (int8)\")\n",
        "                print('='*length_interval)\n",
        "\n",
        "    print()\n",
        "    memory_after_compress = memory_usage(data, 0)\n",
        "    print(\"Compress Rate: [{0:.2%}]\".format((memory_before_compress-memory_after_compress) / memory_before_compress))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "PiFxiW4oQGQE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "\n",
        "seed = 71\n",
        "\n",
        "def seed_numpy_and_pytorch(s):\n",
        "    random.seed(s)\n",
        "    os.environ['PYTHONHASHSEED'] = str(s)\n",
        "    np.random.seed(s)\n",
        "    torch.manual_seed(s)\n",
        "    torch.cuda.manual_seed(s)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_numpy_and_pytorch(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0f1f0ae12955ccb81577778d02e4aff9b5609b56",
        "id": "xEF8qexVQGQJ",
        "colab_type": "code",
        "outputId": "7ab9cfee-f1eb-413e-d760-99b36e4081a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "\n",
        "train_id = train.ID_code\n",
        "test_id = test.ID_code\n",
        "target = train.target\n",
        "\n",
        "test['target'] = np.nan\n",
        "\n",
        "train = train.append(test).reset_index() # merge train and test\n",
        "\n",
        "train.drop(columns=[\"index\"], inplace=True)\n",
        "\n",
        "del test\n",
        "gc.collect()\n",
        "print('Done, shape=',np.shape(train))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done, shape= (400000, 202)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "b2daf5bb4ba0b601b0829fa5817aa046edc02c70",
        "id": "K-k9LzlcQGQL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# i'm doing this cause i don't know if some keras backend have threading problems...\n",
        "import threading\n",
        "class ReadWriteLock:\n",
        "    def __init__(self):\n",
        "        self._read_ready = threading.Condition(threading.Lock())\n",
        "        self._readers = 0\n",
        "    def acquire_read(self):\n",
        "        self._read_ready.acquire()\n",
        "        try:\n",
        "            self._readers += 1\n",
        "        finally:\n",
        "            self._read_ready.release()\n",
        "    def release_read(self):\n",
        "        self._read_ready.acquire()\n",
        "        try:\n",
        "            self._readers -= 1\n",
        "            if not self._readers:\n",
        "                self._read_ready.notifyAll()\n",
        "        finally:\n",
        "            self._read_ready.release()\n",
        "    def acquire_write(self):\n",
        "        self._read_ready.acquire()\n",
        "        while self._readers > 0:\n",
        "            self._read_ready.wait()\n",
        "    def release_write(self):\n",
        "        self._read_ready.release()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2bf8a8a26fef91a7fcea5e6e0dc4b107434fe1f5",
        "id": "6IgnF53mQGQN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from math import ceil\n",
        "class DAESequence(Sequence):\n",
        "    def __init__(self, df, batch_size=512, random_cols=.15, random_rows=1, use_cache=False, use_lock=False, verbose=True):\n",
        "        self.df = df.values.copy()     # ndarray baby\n",
        "        self.batch_size = int(batch_size)\n",
        "        self.len_data = df.shape[0]\n",
        "        self.len_input_columns = df.shape[1]\n",
        "        if(random_cols <= 0):\n",
        "            self.random_cols = 0\n",
        "        elif(random_cols >= 1):\n",
        "            self.random_cols = self.len_input_columns\n",
        "        else:\n",
        "            self.random_cols = int(random_cols*self.len_input_columns)\n",
        "        if(self.random_cols > self.len_input_columns):\n",
        "            self.random_cols = self.len_input_columns\n",
        "        self.random_rows = random_rows\n",
        "        self.cache = None\n",
        "        self.use_cache = use_cache\n",
        "        self.use_lock = use_lock\n",
        "        self.verbose = verbose\n",
        "        \n",
        "        self.lock = ReadWriteLock()\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if(not self.use_cache):\n",
        "            return\n",
        "        if(self.use_lock):\n",
        "            self.lock.acquire_write()\n",
        "        if(self.verbose):\n",
        "            print(\"Doing Cache\")\n",
        "        self.cache = {}\n",
        "        for i in range(0, self.__len__()):\n",
        "            self.cache[i] = self.__getitem__(i, True)\n",
        "        if(self.use_lock):\n",
        "            self.lock.release_write()\n",
        "        gc.collect()\n",
        "        if(self.verbose):\n",
        "            print(\"Done\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(ceil(self.len_data / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx, doing_cache=False):\n",
        "        if(not doing_cache and self.cache is not None and not (self.random_cols <=0 or self.random_rows<=0)):\n",
        "            if(idx in self.cache.keys()):\n",
        "                if(self.use_lock):\n",
        "                    self.lock.acquire_read()\n",
        "                ret0, ret1 = self.cache[idx][0], self.cache[idx][1]\n",
        "                if(self.use_lock):\n",
        "                    self.lock.release_read()\n",
        "                if (not doing_cache and self.verbose):\n",
        "                    print('DAESequence Cache ', idx)\n",
        "                return ret0, ret1\n",
        "        idx_end = min(idx + self.batch_size, self.len_data)\n",
        "        cur_len = idx_end - idx\n",
        "        rows_to_sample = int(self.random_rows * cur_len)\n",
        "        input_x = self.df[idx: idx_end]\n",
        "        if (self.random_cols <= 0 or self.random_rows <= 0 or rows_to_sample<=0):\n",
        "            return input_x, input_x # not dae\n",
        "        # here start the magic\n",
        "        random_rows = np.random.randint(low=0, high=self.len_data-rows_to_sample, size=rows_to_sample)\n",
        "        random_rows[random_rows>idx] += cur_len # just to don't select twice the current rows\n",
        "        cols_to_shuffle = np.random.randint(low=0, high=self.len_input_columns, size=self.random_cols)\n",
        "        noise_x = input_x.copy()\n",
        "        noise_x[0:rows_to_sample, cols_to_shuffle] = self.df[random_rows[:,None], cols_to_shuffle]\n",
        "        if(not doing_cache and self.verbose):\n",
        "            print('DAESequence ', idx)\n",
        "        return noise_x, input_x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3ecb71183c9d2dca030ccc0346cc84535a88cde9",
        "id": "YKHYDIG_QGQP",
        "colab_type": "code",
        "outputId": "f495c37c-0010-4503-8e35-f053af4d80ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Create Model\")\n",
        "dae_data = train[train.columns.drop([\"ID_code\",'target'])] # only get \"X\" vector\n",
        "\n",
        "# reduce data size, we are in kaggle =)\n",
        "\n",
        "len_input_columns, len_data = dae_data.shape[1], dae_data.shape[0]\n",
        "NUM_GPUS=1\n",
        "#kernel_initializer='Orthogonal'  # this one give non NaN more often than others \n",
        "\n",
        "# from https://kaggle2.blob.core.windows.net/forum-message-attachments/250927/8325/nn.cfg.log\n",
        "#L0: 221(in)-1500 'r'ReLU  lRate:0.003 lRateDecay:0.995 regL2:0 regL1:0 dropout:0  w:222x1500  out(x3):1501x128 (0.00210051 GB) init..(uni:1 sp:1)[min|max|mean|std:-0.0672672|0.0672671|-4.74564e-05|0.0388202]\n",
        "#L1: 1500(in)-1500 'r'ReLU  lRate:0.003 lRateDecay:0.995 regL2:0 regL1:0 dropout:0  w:1501x1500  out(x3):1501x128 (0.00977451 GB) init..(uni:1 sp:1)[min|max|mean|std:-0.0258199|0.0258199|8.51905e-06|0.0148989]\n",
        "#L2: 1500(in)-1500 'r'ReLU  lRate:0.003 lRateDecay:0.995 regL2:0 regL1:0 dropout:0  w:1501x1500  out(x3):1501x128 (0.00977451 GB) init..(uni:1 sp:1)[min|max|mean|std:-0.0258199|0.0258199|8.51905e-06|0.0148989]\n",
        "#L3: 1500(in)-221 'l'linear  lRate:0.003 lRateDecay:0.995 regL2:0 regL1:0 dropout:0  w:1501x221  out(x3):222x128 (0.00144055 GB) init..(uni:1 sp:1)[min|max|mean|std:-0.0258199|0.0258198|-1.80977e-05|0.0149005]\n",
        "\n",
        "# is uni:1 = uniform? what about sp:1 ?\n",
        "# std ~= sqrt(2 / (input+output))   (first layer)\n",
        "# std ~= sqrt(1 / (input+output))   (others layer)\n",
        "kernel_initializer_0=keras.initializers.RandomNormal(mean=-4.74564e-05, stddev=0.0388202, seed=None)   # sqrt(2/(221+1500)) = 0.0341 vs 0,0388\n",
        "kernel_initializer_1=keras.initializers.RandomNormal(mean=8.51905e-06, stddev=0.0148989, seed=None)    # sqrt(1/(1500+1500)) = 0.018 vs 0,014\n",
        "kernel_initializer_3=keras.initializers.RandomNormal(mean=-1.80977e-05, stddev=0.0149005, seed=None)   # sqrt(1/(1500+221)) = 0.024 vs 0,014\n",
        "\n",
        "print(\"Input len=\", len_input_columns, len_data)\n",
        "model_dae = Sequential()\n",
        "model_dae.add(Dense(units=len_input_columns*2, activation='relu', dtype='float32', name='Hidden1', input_shape=(len_input_columns,), kernel_initializer=kernel_initializer_0))\n",
        "model_dae.add(Dense(units=len_input_columns, activation='linear', dtype='float16', name='Output', kernel_initializer=kernel_initializer_3))\n",
        "model_opt = keras.optimizers.SGD(lr=0.003, decay=1-0.995, momentum=0, nesterov=False) # decay -> Oscar Takeshita comment\n",
        "\n",
        "try:\n",
        "    print('Loading model from file')\n",
        "    model_dae = keras.models.load_model('DAE.keras.model.h5')\n",
        "except Exception as e:\n",
        "    print(\"Can't load previous fitting parameters and model\", repr(e))\n",
        "if(NUM_GPUS>1):\n",
        "    try:\n",
        "        multi_gpu_model = keras.utils.multi_gpu_model(model_dae, gpus=NUM_GPUS)\n",
        "        multi_gpu_model.compile(loss='mean_squared_error', optimizer=model_opt)\n",
        "        print(\"MULTI GPU MODEL\")\n",
        "        print(multi_gpu_model.summary())\n",
        "    except Exception as e:\n",
        "        print(\"Can't run multi gpu, error=\", repr(e))\n",
        "        model_dae.compile(loss='mean_squared_error', optimizer=model_opt)\n",
        "        NUM_GPUS=0\n",
        "else:\n",
        "    model_dae.compile(loss='mean_squared_error', optimizer=model_opt)\n",
        "\n",
        "print(\"BASE MODEL\")\n",
        "print(model_dae.summary())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create Model\n",
            "Input len= 200 400000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Loading model from file\n",
            "Can't load previous fitting parameters and model OSError(\"Unable to open file (unable to open file: name = 'DAE.keras.model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\",)\n",
            "BASE MODEL\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Hidden1 (Dense)              (None, 400)               80400     \n",
            "_________________________________________________________________\n",
            "Output (Dense)               (None, 200)               80200     \n",
            "=================================================================\n",
            "Total params: 160,600\n",
            "Trainable params: 160,600\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "83b1e974b9d7a35c07db62e1b98d284a64740faf",
        "id": "IH5uuBiqQGQR",
        "colab_type": "code",
        "outputId": "1f602f6d-ebed-44eb-f088-d4dd257dfa5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17751
        }
      },
      "cell_type": "code",
      "source": [
        "from math import ceil\n",
        "batch_size = 512\n",
        "multi_process_workers = 2\n",
        "if (NUM_GPUS > 1):\n",
        "    multi_gpu_model.fit_generator(\n",
        "        DAESequence(dae_data, batch_size=batch_size*NUM_GPUS, verbose=False),\n",
        "        steps_per_epoch=int(ceil(dae_data.shape[0]/(batch_size*NUM_GPUS))),\n",
        "        workers=multi_process_workers, use_multiprocessing=True if multi_process_workers>1 else False,\n",
        "        epochs=500,\n",
        "        verbose=1,\n",
        "        callbacks=[\n",
        "            # keras.callbacks.LambdaCallback(on_epoch_end=lambda x,y: model_dae.save('DAE.keras.model.h5')) # save weights \n",
        "        ])\n",
        "else: # single CPU/GPU\n",
        "    model_dae.fit_generator(\n",
        "        DAESequence(dae_data, batch_size=batch_size, verbose=False),\n",
        "        steps_per_epoch=int(ceil(dae_data.shape[0]/batch_size)),\n",
        "        epochs=500,\n",
        "        workers=multi_process_workers, use_multiprocessing=True if multi_process_workers>1 else False,\n",
        "        verbose=1, callbacks=[\n",
        "            # keras.callbacks.LambdaCallback(on_epoch_end=lambda x,y: model_dae.save('DAE.keras.model.h5')) # save weights\n",
        "        ])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/500\n",
            "782/782 [==============================] - 9s 11ms/step - loss: 24.3453\n",
            "Epoch 2/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 18.5948\n",
            "Epoch 3/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 16.8585\n",
            "Epoch 4/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 15.9278\n",
            "Epoch 5/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 15.3261\n",
            "Epoch 6/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 14.8939\n",
            "Epoch 7/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 14.5637\n",
            "Epoch 8/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 14.2938\n",
            "Epoch 9/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 14.0748\n",
            "Epoch 10/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 13.8932\n",
            "Epoch 11/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 13.7353\n",
            "Epoch 12/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 13.6054\n",
            "Epoch 13/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 13.4810\n",
            "Epoch 14/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 13.3701\n",
            "Epoch 15/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 13.2719\n",
            "Epoch 16/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 13.1815\n",
            "Epoch 17/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 13.0981\n",
            "Epoch 18/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 13.0236\n",
            "Epoch 18/500\n",
            "Epoch 19/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 12.9655\n",
            "Epoch 20/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 12.9038\n",
            "Epoch 21/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 12.8319\n",
            "Epoch 22/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 12.7891\n",
            "Epoch 23/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 12.7459\n",
            "Epoch 24/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 12.6809\n",
            "Epoch 25/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 12.6374\n",
            "Epoch 26/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 12.6028\n",
            "Epoch 27/500\n",
            "\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 12.5595\n",
            "Epoch 28/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 12.5297\n",
            "Epoch 29/500\n",
            "\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 12.4847\n",
            "Epoch 30/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 12.4509\n",
            "Epoch 31/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 12.4847\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 12.4288\n",
            "Epoch 32/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 12.3776\n",
            "Epoch 33/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 12.3527\n",
            "Epoch 34/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 12.3295\n",
            "Epoch 35/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 12.2922\n",
            "Epoch 36/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 12.2740\n",
            "Epoch 37/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 12.2398\n",
            "Epoch 38/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 12.2175\n",
            "Epoch 39/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 12.1887\n",
            "Epoch 40/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 12.1682\n",
            "Epoch 41/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 12.1499\n",
            "Epoch 42/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 12.1288\n",
            "Epoch 43/500\n",
            "\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 12.1079\n",
            "Epoch 44/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 12.0922\n",
            "Epoch 45/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 12.0747\n",
            "Epoch 46/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 12.0489\n",
            "Epoch 47/500\n",
            "Epoch 46/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 12.0338\n",
            "Epoch 48/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 12.0069\n",
            "Epoch 49/500\n",
            "Epoch 48/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 12.0022\n",
            "Epoch 50/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.9783\n",
            "Epoch 51/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.9633\n",
            "Epoch 52/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.9506\n",
            "Epoch 53/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.9345\n",
            "Epoch 54/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.9151\n",
            "Epoch 55/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.9114\n",
            "Epoch 56/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.8774\n",
            "Epoch 57/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.8743\n",
            "Epoch 58/500\n",
            "\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.8616\n",
            "Epoch 59/500\n",
            "782/782 [==============================] - 8s 11ms/step - loss: 11.8489\n",
            "Epoch 60/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.8347\n",
            "Epoch 61/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.8232\n",
            "Epoch 62/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.8119\n",
            "Epoch 63/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.8004\n",
            "Epoch 64/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.7841\n",
            "Epoch 65/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.7781\n",
            "Epoch 66/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.7701\n",
            "Epoch 67/500\n",
            "Epoch 66/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.7528\n",
            "Epoch 68/500\n",
            "782/782 [==============================] - 7s 10ms/step - loss: 11.7370\n",
            "Epoch 69/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.7316\n",
            "Epoch 70/500\n",
            "782/782 [==============================] - 7s 10ms/step - loss: 11.7325\n",
            "Epoch 71/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.7156\n",
            "Epoch 72/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.7059\n",
            "Epoch 73/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.6843\n",
            "Epoch 74/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.6775\n",
            "Epoch 75/500\n",
            "782/782 [==============================] - 7s 10ms/step - loss: 11.6669\n",
            "Epoch 76/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.6656\n",
            "Epoch 77/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.6492\n",
            "Epoch 78/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.6404\n",
            "Epoch 79/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.6383\n",
            "Epoch 80/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.6261\n",
            "Epoch 81/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.6117\n",
            "Epoch 82/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.6042\n",
            "Epoch 83/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.6082\n",
            "Epoch 84/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.5890\n",
            "Epoch 85/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.5945\n",
            "Epoch 86/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.5837\n",
            "Epoch 87/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.5687\n",
            "Epoch 88/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.5661\n",
            "Epoch 89/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.5466\n",
            "Epoch 90/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.5572\n",
            "Epoch 91/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.5373\n",
            "Epoch 92/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.5367\n",
            "Epoch 93/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.5321\n",
            "Epoch 94/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.5183\n",
            "Epoch 95/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.5193\n",
            "Epoch 96/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.5124\n",
            "Epoch 97/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.4993\n",
            "Epoch 98/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.4936\n",
            "Epoch 99/500\n",
            "782/782 [==============================] - 7s 10ms/step - loss: 11.4830\n",
            "Epoch 100/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.4751\n",
            "Epoch 101/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.4819\n",
            "Epoch 102/500\n",
            "782/782 [==============================] - 7s 10ms/step - loss: 11.4715\n",
            "Epoch 103/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.4608\n",
            "Epoch 104/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.4547\n",
            "Epoch 105/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.4470\n",
            "Epoch 106/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.4467\n",
            "Epoch 107/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.4347\n",
            "Epoch 108/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.4278\n",
            "Epoch 109/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.4271\n",
            "Epoch 110/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.4262\n",
            "Epoch 111/500\n",
            "782/782 [==============================] - 7s 10ms/step - loss: 11.4188\n",
            "Epoch 112/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.4190\n",
            "Epoch 113/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.4022\n",
            "Epoch 114/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.3980\n",
            "Epoch 115/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.3897\n",
            "Epoch 116/500\n",
            "782/782 [==============================] - 8s 11ms/step - loss: 11.3873\n",
            "Epoch 117/500\n",
            "\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.3904\n",
            "Epoch 118/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.3809\n",
            "Epoch 119/500\n",
            "\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.3641\n",
            "Epoch 120/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.3713\n",
            "Epoch 121/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.3654\n",
            "Epoch 122/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.3596\n",
            "Epoch 123/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.3506\n",
            "Epoch 124/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.3536\n",
            "Epoch 125/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.3442\n",
            "Epoch 126/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.3368\n",
            "Epoch 127/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.3315\n",
            "Epoch 128/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.3396\n",
            "Epoch 129/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.3261\n",
            "Epoch 130/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.3255\n",
            "Epoch 131/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.3216\n",
            "Epoch 132/500\n",
            "Epoch 131/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.3084\n",
            "Epoch 133/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.3066\n",
            "Epoch 134/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.3049\n",
            "Epoch 135/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.3036\n",
            "Epoch 136/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.2943\n",
            "Epoch 137/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.2919\n",
            "Epoch 138/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.2803\n",
            "Epoch 139/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.2860\n",
            "Epoch 140/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.2741\n",
            "Epoch 141/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.2782\n",
            "Epoch 142/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.2697\n",
            "Epoch 143/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.2682\n",
            "Epoch 144/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.2691\n",
            "Epoch 145/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.2674\n",
            "Epoch 146/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.2534\n",
            "Epoch 147/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.2512\n",
            "Epoch 148/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.2480\n",
            "Epoch 149/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.2455\n",
            "Epoch 150/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.2477\n",
            "Epoch 151/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.2423\n",
            "Epoch 152/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.2354\n",
            "Epoch 153/500\n",
            "\n",
            "782/782 [==============================] - 7s 10ms/step - loss: 11.2234\n",
            "Epoch 154/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.2297\n",
            "Epoch 155/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.2294\n",
            "Epoch 156/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.2194\n",
            "Epoch 157/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.2132\n",
            "Epoch 158/500\n",
            "782/782 [==============================] - 7s 10ms/step - loss: 11.2062\n",
            "Epoch 159/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.2092\n",
            "Epoch 160/500\n",
            "782/782 [==============================] - 7s 10ms/step - loss: 11.2049\n",
            "Epoch 161/500\n",
            "782/782 [==============================] - 7s 10ms/step - loss: 11.1997\n",
            "Epoch 162/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.1897\n",
            "Epoch 163/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.1910\n",
            "Epoch 164/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.1888\n",
            "Epoch 165/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.1930\n",
            "Epoch 166/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.1908\n",
            "Epoch 167/500\n",
            "782/782 [==============================] - 7s 10ms/step - loss: 11.1826\n",
            "Epoch 168/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.1775\n",
            "Epoch 169/500\n",
            "Epoch 168/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.1653\n",
            "Epoch 170/500\n",
            "\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.1653\n",
            "Epoch 171/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.1622\n",
            "Epoch 172/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.1616\n",
            "Epoch 173/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.1744\n",
            "Epoch 174/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.1616\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.1628\n",
            "Epoch 175/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.1595\n",
            "Epoch 176/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.1515\n",
            "Epoch 177/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.1440\n",
            "Epoch 178/500\n",
            "782/782 [==============================] - 7s 10ms/step - loss: 11.1587\n",
            "Epoch 179/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.1472\n",
            "Epoch 180/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 11.1456\n",
            "Epoch 181/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.1393\n",
            "Epoch 182/500\n",
            "782/782 [==============================] - 9s 11ms/step - loss: 11.1464\n",
            "Epoch 183/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.1282\n",
            "Epoch 184/500\n",
            "782/782 [==============================] - 9s 11ms/step - loss: 11.1237\n",
            "Epoch 185/500\n",
            "782/782 [==============================] - 8s 11ms/step - loss: 11.1173\n",
            "Epoch 186/500\n",
            "782/782 [==============================] - 9s 12ms/step - loss: 11.1175\n",
            "Epoch 187/500\n",
            "782/782 [==============================] - 8s 11ms/step - loss: 11.1172\n",
            "Epoch 188/500\n",
            "782/782 [==============================] - 9s 11ms/step - loss: 11.1178\n",
            "Epoch 189/500\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 11.1107\n",
            "Epoch 190/500\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 11.1153\n",
            "Epoch 191/500\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 11.1085\n",
            "Epoch 192/500\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 11.1046\n",
            "Epoch 193/500\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 11.1030\n",
            "Epoch 194/500\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 11.0961\n",
            "Epoch 195/500\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 11.1061\n",
            "Epoch 196/500\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 11.0900\n",
            "Epoch 197/500\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 11.0938\n",
            "Epoch 198/500\n",
            "782/782 [==============================] - 9s 12ms/step - loss: 11.0896\n",
            "Epoch 199/500\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 11.0969\n",
            "Epoch 200/500\n",
            "782/782 [==============================] - 8s 11ms/step - loss: 11.0950\n",
            "Epoch 201/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0791\n",
            "Epoch 202/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0852\n",
            "Epoch 203/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0841\n",
            "Epoch 204/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0688\n",
            "Epoch 205/500\n",
            "\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0753\n",
            "Epoch 206/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0760\n",
            "Epoch 207/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0620\n",
            "Epoch 208/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0684\n",
            "Epoch 209/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0728\n",
            "Epoch 210/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0641\n",
            "Epoch 211/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0615\n",
            "Epoch 212/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0547\n",
            "Epoch 213/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0653\n",
            "Epoch 214/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0450\n",
            "Epoch 215/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0408\n",
            "Epoch 216/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0440\n",
            "Epoch 217/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0463\n",
            "Epoch 218/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0397\n",
            "Epoch 219/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0399\n",
            "Epoch 220/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0394\n",
            "Epoch 221/500\n",
            "Epoch 220/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0480\n",
            "Epoch 222/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0266\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 223/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0204\n",
            "Epoch 224/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0200\n",
            "Epoch 225/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0306\n",
            "Epoch 226/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0227\n",
            "Epoch 227/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0206\n",
            "Epoch 228/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0169\n",
            "Epoch 229/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0140\n",
            "Epoch 230/500\n",
            "\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0233\n",
            "Epoch 231/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0071\n",
            "Epoch 232/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0176\n",
            "Epoch 233/500\n",
            "\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0150\n",
            "Epoch 234/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0083\n",
            "Epoch 235/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0101\n",
            "Epoch 236/500\n",
            "\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0038\n",
            "Epoch 237/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0036\n",
            "Epoch 238/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9953\n",
            "Epoch 239/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0004\n",
            "Epoch 240/500\n",
            "\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 11.0013\n",
            "Epoch 241/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9947\n",
            "Epoch 242/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9938\n",
            "Epoch 243/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9925\n",
            "Epoch 244/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9921\n",
            "Epoch 245/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9963\n",
            "Epoch 246/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9919\n",
            "Epoch 247/500\n",
            "782/782 [==============================] - 8s 11ms/step - loss: 10.9739\n",
            "Epoch 248/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9831\n",
            "Epoch 249/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9774\n",
            "Epoch 250/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9907\n",
            "Epoch 251/500\n",
            "Epoch 250/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9715\n",
            "Epoch 252/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9719\n",
            "Epoch 253/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9632\n",
            "Epoch 254/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9607\n",
            "Epoch 255/500\n",
            "782/782 [==============================] - 8s 11ms/step - loss: 10.9713\n",
            "Epoch 256/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9692\n",
            "Epoch 257/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9713\n",
            "Epoch 258/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9638\n",
            "Epoch 259/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9645\n",
            "Epoch 260/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9615\n",
            "Epoch 261/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9508\n",
            "Epoch 262/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9533\n",
            "Epoch 263/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9530\n",
            "Epoch 264/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9522\n",
            "Epoch 265/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9462\n",
            "Epoch 265/500\n",
            "Epoch 266/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9458\n",
            "Epoch 266/500\n",
            "Epoch 267/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9505\n",
            "Epoch 268/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9343\n",
            "Epoch 269/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9443\n",
            "Epoch 270/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9435\n",
            "Epoch 271/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9496\n",
            "Epoch 272/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9330\n",
            "Epoch 273/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9266\n",
            "Epoch 274/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9319\n",
            "Epoch 275/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9279\n",
            "Epoch 276/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9293\n",
            "Epoch 277/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9256\n",
            "Epoch 278/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9240\n",
            "Epoch 279/500\n",
            "\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9240\n",
            "Epoch 280/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9150\n",
            "Epoch 281/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9256\n",
            "Epoch 282/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9217\n",
            "Epoch 283/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9247\n",
            "Epoch 284/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9176\n",
            "Epoch 285/500\n",
            "Epoch 284/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9129\n",
            "Epoch 286/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9211\n",
            "Epoch 287/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.9068\n",
            "Epoch 288/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9079\n",
            "Epoch 289/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.9070\n",
            "Epoch 290/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.9030\n",
            "Epoch 291/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.9062\n",
            "Epoch 292/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.9001\n",
            "Epoch 293/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.9061\n",
            "Epoch 294/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.8982\n",
            "Epoch 295/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.9023\n",
            "Epoch 296/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8903\n",
            "Epoch 297/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.9007\n",
            "Epoch 298/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.8940\n",
            "Epoch 299/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.8935\n",
            "Epoch 300/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8995\n",
            "Epoch 301/500\n",
            "\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8876\n",
            "Epoch 302/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8917\n",
            "Epoch 303/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8823\n",
            "Epoch 304/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8821\n",
            "Epoch 305/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8887\n",
            "Epoch 306/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.8851\n",
            "Epoch 307/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.8751\n",
            "Epoch 308/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8813\n",
            "Epoch 309/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8783\n",
            "Epoch 310/500\n",
            "782/782 [==============================] - 7s 10ms/step - loss: 10.8825\n",
            "Epoch 311/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8717\n",
            "Epoch 312/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8766\n",
            "Epoch 313/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8667\n",
            "Epoch 314/500\n",
            "782/782 [==============================] - 7s 10ms/step - loss: 10.8646\n",
            "Epoch 315/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8683\n",
            "Epoch 316/500\n",
            "782/782 [==============================] - 7s 10ms/step - loss: 10.8670\n",
            "Epoch 317/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8672\n",
            "Epoch 318/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8669\n",
            "Epoch 319/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8710\n",
            "Epoch 320/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8598\n",
            "Epoch 321/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8562\n",
            "Epoch 322/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8585\n",
            "Epoch 323/500\n",
            "782/782 [==============================] - 7s 10ms/step - loss: 10.8526\n",
            "Epoch 324/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8510\n",
            "Epoch 325/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.8558\n",
            "Epoch 326/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.8641\n",
            "Epoch 327/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.8497\n",
            "Epoch 328/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.8489\n",
            "Epoch 329/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8508\n",
            "Epoch 330/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8449\n",
            "Epoch 331/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.8467\n",
            "Epoch 332/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.8444\n",
            "Epoch 333/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.8522\n",
            "Epoch 334/500\n",
            "Epoch 333/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.8429\n",
            "Epoch 335/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.8531\n",
            "Epoch 336/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8445\n",
            "Epoch 337/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8410\n",
            "Epoch 338/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8419\n",
            "Epoch 339/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8319\n",
            "Epoch 340/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.8304\n",
            "Epoch 341/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8365\n",
            "Epoch 342/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8348\n",
            "Epoch 343/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8300\n",
            "Epoch 344/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8281\n",
            "Epoch 345/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8362\n",
            "Epoch 346/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8227\n",
            "Epoch 347/500\n",
            "782/782 [==============================] - 7s 10ms/step - loss: 10.8320\n",
            "Epoch 348/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8243\n",
            "Epoch 349/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8266\n",
            "Epoch 350/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8306\n",
            "Epoch 351/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8151\n",
            "Epoch 352/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8220\n",
            "Epoch 353/500\n",
            "\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8214\n",
            "Epoch 354/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8201\n",
            "Epoch 355/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8162\n",
            "Epoch 356/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8163\n",
            "Epoch 357/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8229\n",
            "Epoch 358/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8003\n",
            "Epoch 359/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.8149\n",
            "Epoch 360/500\n",
            "782/782 [==============================] - 7s 10ms/step - loss: 10.8080\n",
            "Epoch 361/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.8070\n",
            "Epoch 362/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.8031\n",
            "Epoch 363/500\n",
            "782/782 [==============================] - 7s 10ms/step - loss: 10.8074\n",
            "Epoch 364/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.8081\n",
            "Epoch 365/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.8022\n",
            "Epoch 366/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.7980\n",
            "Epoch 367/500\n",
            "782/782 [==============================] - 7s 10ms/step - loss: 10.7954\n",
            "Epoch 368/500\n",
            "782/782 [==============================] - 7s 10ms/step - loss: 10.8057\n",
            "Epoch 369/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.8021\n",
            "Epoch 370/500\n",
            "782/782 [==============================] - 7s 10ms/step - loss: 10.7945\n",
            "Epoch 371/500\n",
            "782/782 [==============================] - 7s 10ms/step - loss: 10.7954\n",
            "Epoch 372/500\n",
            "782/782 [==============================] - 7s 10ms/step - loss: 10.7859\n",
            "Epoch 373/500\n",
            "782/782 [==============================] - 7s 10ms/step - loss: 10.7990\n",
            "Epoch 374/500\n",
            "\n",
            "782/782 [==============================] - 7s 10ms/step - loss: 10.8009\n",
            "Epoch 375/500\n",
            "782/782 [==============================] - 7s 10ms/step - loss: 10.7948\n",
            "Epoch 376/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7930\n",
            "Epoch 377/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.7814\n",
            "Epoch 378/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.7931\n",
            "Epoch 379/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.7859\n",
            "Epoch 380/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.7822\n",
            "Epoch 381/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7862\n",
            "Epoch 382/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7778\n",
            "Epoch 383/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.7906\n",
            "Epoch 384/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.7794\n",
            "Epoch 385/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.7873\n",
            "Epoch 386/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.7820\n",
            "Epoch 387/500\n",
            "\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.7780\n",
            "Epoch 388/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.7796\n",
            "Epoch 389/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.7732\n",
            "Epoch 390/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.7631\n",
            "Epoch 391/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.7703\n",
            "Epoch 392/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.7739\n",
            "Epoch 393/500\n",
            "782/782 [==============================] - 8s 11ms/step - loss: 10.7736\n",
            "Epoch 394/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7706\n",
            "Epoch 395/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7649\n",
            "Epoch 396/500\n",
            "782/782 [==============================] - 9s 11ms/step - loss: 10.7816\n",
            "Epoch 397/500\n",
            "782/782 [==============================] - 9s 12ms/step - loss: 10.7761\n",
            "Epoch 398/500\n",
            "782/782 [==============================] - 9s 11ms/step - loss: 10.7639\n",
            "Epoch 399/500\n",
            "782/782 [==============================] - 9s 11ms/step - loss: 10.7676\n",
            "Epoch 400/500\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 10.7719\n",
            "Epoch 401/500\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 10.7617\n",
            "Epoch 402/500\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 10.7609\n",
            "Epoch 403/500\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 10.7624\n",
            "Epoch 404/500\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 10.7664\n",
            "Epoch 405/500\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 10.7616\n",
            "Epoch 406/500\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 10.7545\n",
            "Epoch 407/500\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 10.7575\n",
            "Epoch 408/500\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 10.7529\n",
            "Epoch 409/500\n",
            "782/782 [==============================] - 10s 12ms/step - loss: 10.7533\n",
            "Epoch 410/500\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 10.7655\n",
            "Epoch 411/500\n",
            "782/782 [==============================] - 9s 12ms/step - loss: 10.7588\n",
            "Epoch 412/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7606\n",
            "Epoch 413/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7568\n",
            "Epoch 414/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7476\n",
            "Epoch 415/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7477\n",
            "Epoch 416/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7536\n",
            "Epoch 417/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7472\n",
            "Epoch 418/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7370\n",
            "Epoch 419/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7485\n",
            "Epoch 420/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7457\n",
            "Epoch 421/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7510\n",
            "Epoch 422/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7556\n",
            "Epoch 423/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7458\n",
            "Epoch 424/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7417\n",
            "Epoch 425/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7403\n",
            "Epoch 426/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7401\n",
            "Epoch 427/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7293\n",
            "Epoch 428/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7373\n",
            "Epoch 429/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7446\n",
            "Epoch 430/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7381\n",
            "Epoch 431/500\n",
            "Epoch 430/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7386\n",
            "Epoch 432/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7415\n",
            "Epoch 433/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7382\n",
            "Epoch 434/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7439\n",
            "Epoch 435/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7252\n",
            "Epoch 436/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7311\n",
            "Epoch 437/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7304\n",
            "Epoch 438/500\n",
            "Epoch 437/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7251\n",
            "Epoch 439/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7369\n",
            "Epoch 440/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7215\n",
            "Epoch 441/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7261\n",
            "Epoch 442/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7307\n",
            "Epoch 443/500\n",
            "\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7328\n",
            "Epoch 444/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7289\n",
            "Epoch 445/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7257\n",
            "Epoch 446/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7244\n",
            "Epoch 447/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7278\n",
            "Epoch 448/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7214\n",
            "Epoch 449/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7134\n",
            "Epoch 450/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7174\n",
            "Epoch 451/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7249\n",
            "Epoch 452/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7094\n",
            "Epoch 453/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7195\n",
            "Epoch 454/500\n",
            "Epoch 453/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7156\n",
            "Epoch 455/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7128\n",
            "Epoch 456/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7084\n",
            "Epoch 457/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7086\n",
            "Epoch 458/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7068\n",
            "Epoch 459/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7122\n",
            "Epoch 460/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7029\n",
            "Epoch 461/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7115\n",
            "Epoch 462/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7172\n",
            "Epoch 463/500\n",
            "\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7073\n",
            "Epoch 464/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7065\n",
            "Epoch 465/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7019\n",
            "Epoch 466/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7018\n",
            "Epoch 467/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7019\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7006\n",
            "Epoch 468/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7037\n",
            "Epoch 469/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7048\n",
            "Epoch 470/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7003\n",
            "Epoch 471/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.6949\n",
            "Epoch 472/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.6970\n",
            "Epoch 473/500\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 472/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.6972\n",
            "Epoch 474/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7120\n",
            "Epoch 475/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.6979\n",
            "Epoch 476/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.6928\n",
            "Epoch 477/500\n",
            "Epoch 476/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7058\n",
            "Epoch 478/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.6873\n",
            "Epoch 479/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.7070\n",
            "Epoch 480/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.6946\n",
            "Epoch 481/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.6946\n",
            "Epoch 482/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.6837\n",
            "Epoch 483/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.6917\n",
            "Epoch 484/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.6945\n",
            "Epoch 485/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.6833\n",
            "Epoch 486/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.6814\n",
            "Epoch 487/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.6966\n",
            "Epoch 488/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.6849\n",
            "Epoch 489/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.6802\n",
            "Epoch 490/500\n",
            "Epoch 489/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.6893\n",
            "Epoch 491/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.6777\n",
            "Epoch 492/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.6891\n",
            "Epoch 493/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.6785\n",
            "Epoch 494/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.6800\n",
            "Epoch 495/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.6838\n",
            "Epoch 496/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.6796\n",
            "Epoch 497/500\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 10.6820\n",
            "Epoch 498/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.6763\n",
            "Epoch 499/500\n",
            "\n",
            "782/782 [==============================] - 7s 10ms/step - loss: 10.6839\n",
            "Epoch 500/500\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 10.6698\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "9130c7244647b8c3938aa72e01b3f940ac8420e6",
        "id": "cI2L6Y7_QGQU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2900a0d1-fd38-4c74-accd-f85222a8c78b"
      },
      "cell_type": "code",
      "source": [
        "#after you have DAE fitted...\n",
        "your_new_df=train[['ID_code','target']].copy()\n",
        "# let's cut it again...\n",
        "# reduce data size, we are in kaggle =)\n",
        "\n",
        "del train\n",
        "gc.collect()\n",
        "\n",
        "for i in ['1']:\n",
        "    print('Hidden layer',i)\n",
        "    columns_names = ['Hidden_'+str(i)+'_'+str(l) for l in range(0, len_input_columns*2)]\n",
        "    for l in columns_names:\n",
        "        your_new_df[l] = 0 # create columns (maybe it's not optimized)\n",
        "    intermediate_layer_model = Model(inputs=model_dae.input, outputs=model_dae.get_layer('Hidden' + i).output)\n",
        "    your_new_df[columns_names] = intermediate_layer_model.predict(dae_data)\n",
        "\n",
        "print('DONE!')\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hidden layer 1\n",
            "DONE!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "09a44ac01b0238b86efaf7d25338b484cef92cbc",
        "id": "S1pjZ20bQGQX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "your_new_df.to_csv('Denoise_autoencode_seed71_400_200_512_1000.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}