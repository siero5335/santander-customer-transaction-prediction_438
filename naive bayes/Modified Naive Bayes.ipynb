{
  "cells": [
    {
      "metadata": {
        "_uuid": "eed591eff5b1839fc2e51585b8fded5f4e2fa06b"
      },
      "cell_type": "markdown",
      "source": "# Modified Naive Bayes scores 0.899 LB - Santander\nIn this kernel we demonstrate that unconstrained Naive Bayes can score 0.899 LB. I call it \"unconstrained\" because it doesn't assume that each variable has a Gaussian distribution like typical Naive Bayes. Instead we allow for arbitrary distributions and we plot these distributions below. I called it \"modified\" because we don't reverse the conditional probabilities.\n\nThis kernel is useful because (1) it shows that an accurate score can be achieved using a simple model that assumes the variables are independent. And (2) this kernel displays interesting EDA which provides insights about the data.\n  \n# Load Data"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np, pandas as pd\ntrain = pd.read_csv('../input/train.csv')\ntrain0 = train[ train['target']==0 ].copy()\ntrain1 = train[ train['target']==1 ].copy()\ntrain.sample(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2782cfea259691785a109f68ba15ba6ba3677dbf"
      },
      "cell_type": "markdown",
      "source": "# Statistical Functions\nBelow are functions to calcuate various statistical things."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f2c3385c3ed178bdebdfc6f044305202af2cb31c"
      },
      "cell_type": "code",
      "source": "# CALCULATE MEANS AND STANDARD DEVIATIONS\ns = [0]*200\nm = [0]*200\nfor i in range(200):\n    s[i] = np.std(train['var_'+str(i)])\n    m[i] = np.mean(train['var_'+str(i)])\n    \n# CALCULATE PROB(TARGET=1 | X)\ndef getp(i,x):\n    c = 3 #smoothing factor\n    a = len( train1[ (train1['var_'+str(i)]>x-s[i]/c)&(train1['var_'+str(i)]<x+s[i]/c) ] ) \n    b = len( train0[ (train0['var_'+str(i)]>x-s[i]/c)&(train0['var_'+str(i)]<x+s[i]/c) ] )\n    if a+b<500: return 0.1 #smoothing factor\n    # RETURN PROBABILITY\n    return a / (a+b)\n    # ALTERNATIVELY RETURN ODDS\n    # return a / b\n    \n# SMOOTH A DISCRETE FUNCTION\ndef smooth(x,st=1):\n    for j in range(st):\n        x2 = np.ones(len(x)) * 0.1\n        for i in range(len(x)-2):\n            x2[i+1] = 0.25*x[i]+0.5*x[i+1]+0.25*x[i+2]\n        x = x2.copy()\n    return x",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1072218fbe8cc9ba4bae3f297c6109231e602c29"
      },
      "cell_type": "markdown",
      "source": "# Display Target Density and Target Probability\nBelow are two plots for each of the 200 variables. The first is the density of `target=1` versus `target=0`. The second gives the probability that `target=1` given different values for `var_k`."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3414b1bcecbe3dd1a9c45ecc1b0befd4c129b828"
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nPicture = True #draw plots\nrmin=-5; rmax=5; res=501\npr = 0.1 * np.ones((200,res))\npr2 = pr.copy()\nxr = np.zeros((200,res))\nxr2 = xr.copy()\nct2 = 0\nfor j in range(50):\n    if Picture: plt.figure(figsize=(15,8))\n    for v in range(4):\n        ct = 0\n        # CALCULATE PROBABILITY FUNCTION FOR VAR\n        for i in np.linspace(rmin,rmax,res):\n            pr[v+4*j,ct] = getp(v+4*j,m[v+4*j]+i*s[v+4*j])\n            xr[v+4*j,ct] = m[v+4*j]+i*s[v+4*j]\n            xr2[v+4*j,ct] = i\n            ct += 1\n        # SMOOTH FUNCTION FOR PRETTIER DISPLAY\n        # BUT USE UNSMOOTHED FUNCTION FOR PREDICTION\n        pr2[v+4*j,:] = smooth(pr[v+4*j,:],50)\n        if Picture:\n            # DISPLAY PROBABILITY FUNCTION\n            plt.subplot(2, 4, ct2%4+5)\n            plt.plot(xr[v+4*j,:],pr2[v+4*j,:],'-')\n            plt.title('P( t=1 | var_'+str(v+4*j)+' )')\n            xx = plt.xlim()\n            # DISPLAY TARGET DENSITIES\n            plt.subplot(2, 4, ct2%4+1)            \n            sns.distplot(train0['var_'+str(v+4*j)], label = 't=0')\n            sns.distplot(train1['var_'+str(v+4*j)], label = 't=1')\n            plt.title('var_'+str(v+4*j))\n            plt.legend()\n            plt.xlim(xx)\n            plt.xlabel('')\n        if (ct2%8==0): print('Showing vars',ct2,'to',ct2+7,'...')\n        ct2 += 1\n    if Picture: plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "94d58d824e91cc91d23f89116641cc166ac4b4cc"
      },
      "cell_type": "markdown",
      "source": "# Target Probability Function\nAbove, the target probability function was calculated for each variable with resolution equal to `standard deviation / 50` from -5 to 5. For example, we know the `Probability ( target=1 | var=x )` for `z-score = -5.00, -4.98, ..., -0.02, 0, 0.02, ..., 4.98, 5.00` where `z-score = (x - var_mean) / (var_standard_deviation)`. The python function below accesses these pre-calculated values from their numpy array."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2c63c17122a81dcd87dd32895063feae74ef836e"
      },
      "cell_type": "code",
      "source": "def getp2(i,x):\n    z = (x-m[i])/s[i]\n    ss = (rmax-rmin)/(res-1)\n    idx = min( (res+1)//2 + (z-ss/2)//ss, res-1)\n    idx = max(idx,0)\n    return pr[i,int(idx)]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ff4198166c66cdce20aadd9f08c50f7f150e9cb9"
      },
      "cell_type": "markdown",
      "source": "# Validation\nWe will ignore the training data's target and make our own prediction for each training observation. Then using our predictions and the true value, we will calculate validation AUC. (It is true that there is a slight leak in this validation method but it is unsignificant.)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f75c6f345d4a097001842dd14d235e88db6a74ea"
      },
      "cell_type": "code",
      "source": "from sklearn.metrics import roc_auc_score\nprint('Calculating 200000 predictions and displaying a few examples...')\npred = [0]*200000; ct = 0\nfor r in train.index:\n    p = 0.1\n    for i in range(200):\n        p *= 10*getp2(i,train.iloc[r,2+i])\n    if ct%25000==0: print('train',r,'has target =',train.iloc[r,1],'and prediction =',p)\n    pred[ct]=p; ct += 1\nprint('###############')\nprint('Validation AUC =',roc_auc_score(train['target'], pred))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "_uuid": "48b80554e810f57dc9a0ac280e510b28cdba517b"
      },
      "cell_type": "code",
      "source": "#https://stackoverflow.com/questions/25009284/how-to-plot-roc-curve-in-python\nfrom sklearn import metrics\nfpr, tpr, threshold = metrics.roc_curve(train['target'], pred)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.figure(figsize=(6,6))\nplt.title('Validation ROC')\nplt.plot(fpr, tpr, 'b', label = 'Val AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d903f8504004b6c26ed82c0947b46ce2d89d6ca8"
      },
      "cell_type": "markdown",
      "source": "# Predict Test and Submit\nNaive Bayes is a simple model. Given observation with `var_0 = 15`, `var_1 = 5`, `var_2 = 10`, etc. We compute the probability that `target=1` by calculating `P(t=1) * P(t=1 | var_0=15)/P(t=1) * P(t=1 | var_1=5)/P(t=1) * P(t=1 | var_2=10)/P(t=1) * ...` where `P(t=1)=0.1` and the other probabilities are computed above by counting occurences in the training data. So each observation has 200 variables and we simply multiply together the 200 target probabilities given by each variable. (In typical Naive Bayes, you use Bayes formula, reverse the probabilities, and find `P(var_0=15 | t=1)`. This is modified Naive Bayes and more intuitive.)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "95cf397006ae36ba5cd4065daa441aad63b416ea"
      },
      "cell_type": "code",
      "source": "test = pd.read_csv('../input/test.csv')\nprint('Calculating 200000 predictions and displaying a few examples...')\npred = [0]*200000; ct = 0\nfor r in test.index:\n    p = 0.1\n    for i in range(200):\n        p *= 10*getp2(i,test.iloc[r,1+i])\n    if ct%25000==0: print('test',r,'has prediction =',p)\n    pred[ct]=p\n    ct += 1\nsub = pd.read_csv('../input/sample_submission.csv')\nsub['target'] = pred\nsub.to_csv('submission.csv',index=False)\nprint('###############')\nprint('Finished. Wrote predictions to submission.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b06bdd0752b8ebcbefffbfefdcaf3d8d9357d693"
      },
      "cell_type": "markdown",
      "source": "# Plot Predictions"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "60a483a10164d79c81cac00d112c60c42dc3a1b5"
      },
      "cell_type": "code",
      "source": "sub.loc[ sub['target']>1 , 'target'] = 1\nb = plt.hist(sub['target'], bins=200)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ae06eb297bc33776556a7dfdde4e331656c043e9"
      },
      "cell_type": "markdown",
      "source": "# Conclusion\nIn conclusion we used modified Naive Bayes to predict Santander Customer transactions. Since we achieved an accurate score of 0.899 LB (which rivals other methods that capture interactions), this demonstrates that there is little or no interaction between the 200 variables. Additionally in this kernel we observed some fascinating EDA which provide insights about the variables. Can this method be improved? Perhaps by adding regularization we can decrease validation AUC and increase LB AUC but I don't think we can score over 0.902 with this method. There are other secrets hiding in the Santander data."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}