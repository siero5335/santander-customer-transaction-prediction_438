{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "'''\n",
    "Credit to this kernel:\n",
    "https://www.kaggle.com/remidi/neural-compression-auto-encoder-lb-0-55/code\n",
    "\n",
    "I do some change and make it work for Santander . It is my first time to use denoising autoencoder.\n",
    "Please provide feedback and upvote if you like it :)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "5cf981436e3f6b2f6110818c77b40f0f6ca2952c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from sklearn.base import BaseEstimator,TransformerMixin, ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np\n",
    "from sklearn.linear_model import ElasticNetCV, LassoLarsCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "import scipy\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "40118570ba97db477c9827dcf73b6f10a3d1eabd"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "seed = 71\n",
    "\n",
    "def seed_numpy_and_pytorch(s):\n",
    "    random.seed(s)\n",
    "    os.environ['PYTHONHASHSEED'] = str(s)\n",
    "    np.random.seed(s)\n",
    "    torch.manual_seed(s)\n",
    "    torch.cuda.manual_seed(s)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_numpy_and_pytorch(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one hot encoded train shape :: (200000, 200)\n",
      "one hot encoded test shape :: (200000, 200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 200000 samples, validate on 200000 samples\n",
      "Epoch 1/4\n",
      " - 19s - loss: -8.6396e-01 - val_loss: -1.1600e+00\n",
      "Epoch 2/4\n",
      " - 19s - loss: -1.3341e+00 - val_loss: -1.4469e+00\n",
      "Epoch 3/4\n",
      " - 19s - loss: -1.5692e+00 - val_loss: -1.6171e+00\n",
      "Epoch 4/4\n",
      " - 18s - loss: -1.7226e+00 - val_loss: -1.7739e+00\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class StackingEstimator(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        self.estimator.fit(X, y, **fit_params)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = check_array(X)\n",
    "        X_transformed = np.copy(X)\n",
    "        # add class probabilities as a synthetic feature\n",
    "        if issubclass(self.estimator.__class__, ClassifierMixin) and hasattr(self.estimator, 'predict_proba'):\n",
    "            X_transformed = np.hstack((self.estimator.predict_proba(X), X))\n",
    "\n",
    "        # add class prodiction as a synthetic feature\n",
    "        X_transformed = np.hstack((np.reshape(self.estimator.predict(X), (-1, 1)), X_transformed))\n",
    "\n",
    "        return X_transformed\n",
    "\n",
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "\n",
    "mix = pd.concat([train, test])\n",
    "\n",
    "mix.drop(['var_90', 'var_161', 'var_88', 'var_100'], 1)\n",
    "\n",
    "# copy from https://www.kaggle.com/mathormad/knowledge-distillation-with-nn-rankgauss\n",
    "class GaussRankScaler():\n",
    "\n",
    "    def __init__( self ):\n",
    "        self.epsilon = 1e-9\n",
    "        self.lower = -1 + self.epsilon\n",
    "        self.upper =  1 - self.epsilon\n",
    "        self.range = self.upper - self.lower\n",
    "\n",
    "    def fit_transform( self, X ):\n",
    "\n",
    "        i = np.argsort( X, axis = 0 )\n",
    "        j = np.argsort( i, axis = 0 )\n",
    "\n",
    "        assert ( j.min() == 0 ).all()\n",
    "        assert ( j.max() == len( j ) - 1 ).all()\n",
    "\n",
    "        j_range = len( j ) - 1\n",
    "        self.divider = j_range / self.range\n",
    "\n",
    "        transformed = j / self.divider\n",
    "        transformed = transformed - self.upper\n",
    "        transformed = scipy.special.erfinv( transformed )\n",
    "        ############\n",
    "        # transformed = transformed - np.mean(transformed)\n",
    "\n",
    "        return transformed\n",
    "\n",
    "target_col='target'\n",
    "id_col='ID_code'\n",
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "id_test = submission[id_col].values\n",
    "# function for auto encoder with a compressed components n_comp = 12\n",
    "def neural_compression_v2(train, test):\n",
    "    dataset = pd.concat([train.drop(target_col, axis=1), test], axis=0)\n",
    "    ids = dataset[id_col]\n",
    "    dataset.drop(id_col, axis=1, inplace=True)\n",
    "    y_train = train[target_col]\n",
    "    \n",
    "    cat_vars = [c for c in dataset.columns if dataset[c].dtype == 'object']\n",
    "    for c in cat_vars:\n",
    "        t_data = pd.get_dummies(dataset[c], prefix=c)\n",
    "        dataset = pd.concat([dataset, t_data], axis=1)\n",
    "\n",
    "    dataset.drop(cat_vars, axis=1, inplace=True)\n",
    "    # We scale both train and test data so that our NN works better.\n",
    "    sc = StandardScaler()\n",
    "#     sc = GaussRankScaler()# Gauss Rank does not work...\n",
    "    sc.fit_transform(dataset)\n",
    "\n",
    "    dataset = sc.fit_transform(dataset)\n",
    "\n",
    "    train = dataset[:train.shape[0]]\n",
    "    test = dataset[train.shape[0]:]\n",
    "\n",
    "    print(\"one hot encoded train shape :: {}\".format(train.shape))\n",
    "    print(\"one hot encoded test shape :: {}\".format(test.shape))\n",
    "    \n",
    "    ''' neural network compression code '''\n",
    "    \n",
    "    import keras\n",
    "    from keras import regularizers\n",
    "    from keras.layers import Input, Dense,BatchNormalization,Dropout\n",
    "    from keras.models import Model\n",
    "    from keras.regularizers import l2\n",
    "    # adding some noise to data before feed them to nn\n",
    "    train = train + 0.5 * np.random.normal(loc=0.0, scale=1.0, size=train.shape) \n",
    "    test = test + 0.5 * np.random.normal(loc=0.0, scale=1.0, size=test.shape)\n",
    "    l2_reg_embedding = 1e-5\n",
    "    print(keras.__version__)\n",
    "    init_dim = train.shape[1]\n",
    "\n",
    "    input_row = Input(shape=(init_dim, ))\n",
    "    encoded = Dense(512, activation='elu',kernel_regularizer=l2(l2_reg_embedding))(input_row)\n",
    "    encoded = Dense(256, activation='elu',kernel_regularizer=l2(l2_reg_embedding))(encoded)\n",
    "    encoded = Dense(128, activation='elu',kernel_regularizer=l2(l2_reg_embedding))(encoded)\n",
    "    encoded = Dense(64, activation='elu',kernel_regularizer=l2(l2_reg_embedding))(encoded)\n",
    "    encoded = Dense(32, activation='elu',kernel_regularizer=l2(l2_reg_embedding))(encoded)\n",
    "\n",
    "    encoded = Dense(16, activation='elu')(encoded)\n",
    "    \n",
    "    decoded = Dense(32, activation='elu',kernel_regularizer=l2(l2_reg_embedding))(encoded)\n",
    "    decoded = Dense(64, activation='elu',kernel_regularizer=l2(l2_reg_embedding))(decoded)\n",
    "    decoded = Dense(128, activation='elu',kernel_regularizer=l2(l2_reg_embedding))(decoded)\n",
    "    decoded = Dense(256, activation='elu',kernel_regularizer=l2(l2_reg_embedding))(decoded)\n",
    "    decoded = Dense(512, activation='elu',kernel_regularizer=l2(l2_reg_embedding))(decoded)\n",
    "    decoded = Dense(init_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "    autoencoder = Model(inputs=input_row, outputs=decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    #we use the train data to train\n",
    "    autoencoder.fit(train, train,\n",
    "                    batch_size=512,verbose=2,\n",
    "                    shuffle=True, validation_data=(test, test), epochs=4)\n",
    "\n",
    "    # compressing the data\n",
    "    encoder = Model(inputs=input_row, outputs=encoded)\n",
    "    train_compress = encoder.predict(train,batch_size=4096)\n",
    "    test_compress = encoder.predict(test,batch_size=4096)\n",
    "\n",
    "    # denoising the data\n",
    "    denoised_train = autoencoder.predict(train,batch_size=4096)\n",
    "    denoised_test = autoencoder.predict(test,batch_size=4096)\n",
    "    \n",
    "    return train_compress, test_compress, denoised_train, denoised_test\n",
    "\n",
    "train_compress, test_compress, denoised_train, denoised_test = neural_compression_v2(train, test)\n",
    "\n",
    "\n",
    "\n",
    "for c in train.columns:\n",
    "    if train[c].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(train[c].values) + list(test[c].values))\n",
    "        train[c] = lbl.transform(list(train[c].values))\n",
    "        test[c] = lbl.transform(list(test[c].values))\n",
    "        \n",
    "\n",
    "n_comp = 12\n",
    "# ICA\n",
    "ica = FastICA(n_components=n_comp, random_state=420)\n",
    "ica2_results_train = ica.fit_transform(train.drop([target_col], axis=1))\n",
    "ica2_results_test = ica.transform(test)\n",
    "\n",
    "# GRP\n",
    "grp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\n",
    "grp_results_train = grp.fit_transform(train.drop([target_col], axis=1))\n",
    "grp_results_test = grp.transform(test)\n",
    "\n",
    "# SRP\n",
    "srp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\n",
    "srp_results_train = srp.fit_transform(train.drop([target_col], axis=1))\n",
    "srp_results_test = srp.transform(test)\n",
    "\n",
    "# FAG\n",
    "fag = FeatureAgglomeration(n_clusters=n_comp, linkage='ward')\n",
    "fag_results_train = fag.fit_transform(train.drop([target_col], axis=1))\n",
    "fag_results_test = fag.transform(test)\n",
    "\n",
    "usable_columns = list(set(train.columns) - set([target_col]))\n",
    "\n",
    "# Append decomposition components to datasets\n",
    "for i in range(1, n_comp + 1):\n",
    "\n",
    "    train['ica_' + str(i)] = ica2_results_train[:, i - 1]\n",
    "    test['ica_' + str(i)] = ica2_results_test[:, i - 1]\n",
    "\n",
    "    train['grp_' + str(i)] = grp_results_train[:, i - 1]\n",
    "    test['grp_' + str(i)] = grp_results_test[:, i - 1]\n",
    "\n",
    "    train['srp_' + str(i)] = srp_results_train[:, i - 1]\n",
    "    test['srp_' + str(i)] = srp_results_test[:, i - 1]\n",
    "    \n",
    "    train['fag_' + str(i)] = fag_results_train[:, i - 1]\n",
    "    test['fag_' + str(i)] = fag_results_test[:, i - 1]\n",
    "\n",
    "for j in range(1, train_compress.shape[1]):\n",
    "    train['aen_' + str(j)] = train_compress[:, j-1]\n",
    "    test['aen_' + str(j)] = test_compress[:, j-1]\n",
    "    train['aen_' + str(j)] = denoised_train[:, j-1]\n",
    "    test['aen_' + str(j)] = denoised_test[:, j-1]\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "y = train[target_col].values\n",
    "\n",
    "\n",
    "# finaltrainset and finaltestset are data to be used only the stacked model (does not contain PCA, SVD... arrays) \n",
    "finaltrainset = train[usable_columns].values\n",
    "finaltestset = test[usable_columns].values\n",
    "\n",
    "#--training & test stratified split\n",
    "np.savetxt('finaltrainset_dae2.csv',finaltrainset,delimiter=',')\n",
    "np.savetxt('finaltrestset_dae2.csv',finaltestset,delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a42338605bd66612d9815cbd973b5d2312ad0ea5"
   },
   "source": [
    "It seems that stacked_pipeline make performance worse. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
