{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "'''\nCredit to this kernel:\nhttps://www.kaggle.com/remidi/neural-compression-auto-encoder-lb-0-55/code\n\nI do some change and make it work for Santander . It is my first time to use denoising autoencoder.\nPlease provide feedback and upvote if you like it :)\n'''"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5cf981436e3f6b2f6110818c77b40f0f6ca2952c"
      },
      "cell_type": "code",
      "source": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nfrom sklearn.base import BaseEstimator,TransformerMixin, ClassifierMixin\nfrom sklearn.preprocessing import LabelEncoder\nimport xgboost as xgb\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np\nfrom sklearn.linear_model import ElasticNetCV, LassoLarsCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.utils import check_array\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.random_projection import SparseRandomProjection\nfrom sklearn.decomposition import PCA, FastICA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import NMF\nfrom sklearn.cluster import FeatureAgglomeration\nimport scipy\nfrom sklearn.ensemble import RandomForestRegressor\nimport random",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "40118570ba97db477c9827dcf73b6f10a3d1eabd"
      },
      "cell_type": "code",
      "source": "import torch\nimport random\n\nseed = 71\n\ndef seed_numpy_and_pytorch(s):\n    random.seed(s)\n    os.environ['PYTHONHASHSEED'] = str(s)\n    np.random.seed(s)\n    torch.manual_seed(s)\n    torch.cuda.manual_seed(s)\n    torch.backends.cudnn.deterministic = True\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nseed_numpy_and_pytorch(seed)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import warnings\nwarnings.filterwarnings('ignore')\n\n\nclass StackingEstimator(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, estimator):\n        self.estimator = estimator\n\n    def fit(self, X, y=None, **fit_params):\n        self.estimator.fit(X, y, **fit_params)\n        return self\n    def transform(self, X):\n        X = check_array(X)\n        X_transformed = np.copy(X)\n        # add class probabilities as a synthetic feature\n        if issubclass(self.estimator.__class__, ClassifierMixin) and hasattr(self.estimator, 'predict_proba'):\n            X_transformed = np.hstack((self.estimator.predict_proba(X), X))\n\n        # add class prodiction as a synthetic feature\n        X_transformed = np.hstack((np.reshape(self.estimator.predict(X), (-1, 1)), X_transformed))\n\n        return X_transformed\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n# copy from https://www.kaggle.com/mathormad/knowledge-distillation-with-nn-rankgauss\nclass GaussRankScaler():\n\n    def __init__( self ):\n        self.epsilon = 1e-9\n        self.lower = -1 + self.epsilon\n        self.upper =  1 - self.epsilon\n        self.range = self.upper - self.lower\n\n    def fit_transform( self, X ):\n\n        i = np.argsort( X, axis = 0 )\n        j = np.argsort( i, axis = 0 )\n\n        assert ( j.min() == 0 ).all()\n        assert ( j.max() == len( j ) - 1 ).all()\n\n        j_range = len( j ) - 1\n        self.divider = j_range / self.range\n\n        transformed = j / self.divider\n        transformed = transformed - self.upper\n        transformed = scipy.special.erfinv( transformed )\n        ############\n        # transformed = transformed - np.mean(transformed)\n\n        return transformed\n\ntarget_col='target'\nid_col='ID_code'\nsubmission = pd.read_csv('../input/sample_submission.csv')\nid_test = submission[id_col].values\n# function for auto encoder with a compressed components n_comp = 12\ndef neural_compression_v2(train, test):\n    dataset = pd.concat([train.drop(target_col, axis=1), test], axis=0)\n    ids = dataset[id_col]\n    dataset.drop(id_col, axis=1, inplace=True)\n    y_train = train[target_col]\n    \n    cat_vars = [c for c in dataset.columns if dataset[c].dtype == 'object']\n    for c in cat_vars:\n        t_data = pd.get_dummies(dataset[c], prefix=c)\n        dataset = pd.concat([dataset, t_data], axis=1)\n\n    dataset.drop(cat_vars, axis=1, inplace=True)\n    # We scale both train and test data so that our NN works better.\n    sc = StandardScaler()\n#     sc = GaussRankScaler()# Gauss Rank does not work...\n    sc.fit_transform(dataset)\n\n    dataset = sc.fit_transform(dataset)\n\n    train = dataset[:train.shape[0]]\n    test = dataset[train.shape[0]:]\n\n    print(\"one hot encoded train shape :: {}\".format(train.shape))\n    print(\"one hot encoded test shape :: {}\".format(test.shape))\n    \n    ''' neural network compression code '''\n    \n    import keras\n    from keras import regularizers\n    from keras.layers import Input, Dense,BatchNormalization,Dropout\n    from keras.models import Model\n    from keras.regularizers import l2\n    # adding some noise to data before feed them to nn\n    train = train + 0.5 * np.random.normal(loc=0.0, scale=1.0, size=train.shape) \n    test = test + 0.5 * np.random.normal(loc=0.0, scale=1.0, size=test.shape)\n    l2_reg_embedding = 1e-5\n    print(keras.__version__)\n    init_dim = train.shape[1]\n\n    input_row = Input(shape=(init_dim, ))\n    encoded = Dense(512, activation='elu',kernel_regularizer=l2(l2_reg_embedding))(input_row)\n    encoded = Dense(256, activation='elu',kernel_regularizer=l2(l2_reg_embedding))(encoded)\n    encoded = Dense(128, activation='elu',kernel_regularizer=l2(l2_reg_embedding))(encoded)\n    encoded = Dense(64, activation='elu',kernel_regularizer=l2(l2_reg_embedding))(encoded)\n    encoded = Dense(32, activation='elu',kernel_regularizer=l2(l2_reg_embedding))(encoded)\n\n    encoded = Dense(16, activation='elu')(encoded)\n    \n    decoded = Dense(32, activation='elu',kernel_regularizer=l2(l2_reg_embedding))(encoded)\n    decoded = Dense(64, activation='elu',kernel_regularizer=l2(l2_reg_embedding))(decoded)\n    decoded = Dense(128, activation='elu',kernel_regularizer=l2(l2_reg_embedding))(decoded)\n    decoded = Dense(256, activation='elu',kernel_regularizer=l2(l2_reg_embedding))(decoded)\n    decoded = Dense(512, activation='elu',kernel_regularizer=l2(l2_reg_embedding))(decoded)\n    decoded = Dense(init_dim, activation='sigmoid')(decoded)\n\n    autoencoder = Model(inputs=input_row, outputs=decoded)\n    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n    #we use the train data to train\n    autoencoder.fit(train, train,\n                    batch_size=512,verbose=2,\n                    shuffle=True, validation_data=(test, test), epochs=4)\n\n    # compressing the data\n    encoder = Model(inputs=input_row, outputs=encoded)\n    train_compress = encoder.predict(train,batch_size=4096)\n    test_compress = encoder.predict(test,batch_size=4096)\n\n    # denoising the data\n    denoised_train = autoencoder.predict(train,batch_size=4096)\n    denoised_test = autoencoder.predict(test,batch_size=4096)\n    \n    return train_compress, test_compress, denoised_train, denoised_test\n\ntrain_compress, test_compress, denoised_train, denoised_test = neural_compression_v2(train, test)\n\n\n\nfor c in train.columns:\n    if train[c].dtype == 'object':\n        lbl = LabelEncoder()\n        lbl.fit(list(train[c].values) + list(test[c].values))\n        train[c] = lbl.transform(list(train[c].values))\n        test[c] = lbl.transform(list(test[c].values))\n        \n\nn_comp = 12\n# ICA\nica = FastICA(n_components=n_comp, random_state=420)\nica2_results_train = ica.fit_transform(train.drop([target_col], axis=1))\nica2_results_test = ica.transform(test)\n\n# GRP\ngrp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\ngrp_results_train = grp.fit_transform(train.drop([target_col], axis=1))\ngrp_results_test = grp.transform(test)\n\n# SRP\nsrp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\nsrp_results_train = srp.fit_transform(train.drop([target_col], axis=1))\nsrp_results_test = srp.transform(test)\n\n# FAG\nfag = FeatureAgglomeration(n_clusters=n_comp, linkage='ward')\nfag_results_train = fag.fit_transform(train.drop([target_col], axis=1))\nfag_results_test = fag.transform(test)\n\nusable_columns = list(set(train.columns) - set([target_col]))\n\n# Append decomposition components to datasets\nfor i in range(1, n_comp + 1):\n\n    train['ica_' + str(i)] = ica2_results_train[:, i - 1]\n    test['ica_' + str(i)] = ica2_results_test[:, i - 1]\n\n    train['grp_' + str(i)] = grp_results_train[:, i - 1]\n    test['grp_' + str(i)] = grp_results_test[:, i - 1]\n\n    train['srp_' + str(i)] = srp_results_train[:, i - 1]\n    test['srp_' + str(i)] = srp_results_test[:, i - 1]\n    \n    train['fag_' + str(i)] = fag_results_train[:, i - 1]\n    test['fag_' + str(i)] = fag_results_test[:, i - 1]\n\nfor j in range(1, train_compress.shape[1]):\n    train['aen_' + str(j)] = train_compress[:, j-1]\n    test['aen_' + str(j)] = test_compress[:, j-1]\n    train['aen_' + str(j)] = denoised_train[:, j-1]\n    test['aen_' + str(j)] = denoised_test[:, j-1]\n    \n    \n    \n\n\ny = train[target_col].values\n\n\n# finaltrainset and finaltestset are data to be used only the stacked model (does not contain PCA, SVD... arrays) \nfinaltrainset = train[usable_columns].values\nfinaltestset = test[usable_columns].values\n\n#--training & test stratified split\nnp.savetxt('finaltrainset_dae2.csv',finaltrainset,delimiter=',')\nnp.savetxt('finaltrestset_dae2.csv',finaltestset,delimiter=',')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a42338605bd66612d9815cbd973b5d2312ad0ea5"
      },
      "cell_type": "markdown",
      "source": "It seems that stacked_pipeline make performance worse. "
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}